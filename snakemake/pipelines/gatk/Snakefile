
# %%
import os
import shutil
import functools
import itertools
import pathlib
from snakemake.io import InputFiles, OutputFiles
from pmbi.snakemake.lib import *
import pmbi.util
from pmbi.plotting import Paneler
import tomllib
import re
from munch import Munch
import glob
import pandas as pd
import numpy as np

# # %%
# def get_fastqs(wildcards, parent, readnum):
#     return sorted(glob.glob(os.path.join(parent, "{samp}*_R{rn}_*.fastq.gz".format(samp=wildcards.sample, rn=readnum))))
#
# # %%
# def at_source(p, config):
#     return os.path.join(config.core.source, p)


# # %%
# def check_config_paths(config):
#     if not os.path.isfile(config.core.genome_reference):
#         raise IOError(f"Genome reference file does not exist: {config.core.genome_reference}")
#     for bl in config.core.blacklists:
#         if not os.path.isfile(bl):
#             raise IOError(f"Blacklist file does not exist: {bl}") 

# %%
config = toml_config(workflow.source_path("config.toml"))
# check_config_paths(config)

FILES = [x for x in os.listdir("fastq_trimmed") if x.endswith("_trim.fastq.gz")]
# RENAME_FILES = [re.sub(config.core.sample_key, r"\1_\2.fastq.gz", x) for x in FILES]

SAMPLES = list(set([pmbi.util.get_substring(x, config.core.sample_pattern) for x in FILES]))
# SMIDS = {k:v for k,v in zip(SAMPLES, list(range(1, len(SAMPLES)+1)))}

# localrules: link_rename_fastq

rule all:
    input:
        expand("depth/{sample}_depth.txt.gz", sample=SAMPLES),
        expand("gvcf/{sample}.g.vcf", sample=SAMPLES),
        expand("fig/{sample}_depth_plot.png", sample=SAMPLES),
        "combined/combined.g.vcf",
        # "combined/combined.vcf",

rule bwa_mem:
    input:
        read1="fastq_trimmed/{sample}_R1_trim.fastq.gz",
        read2="fastq_trimmed/{sample}_R2_trim.fastq.gz",
    output:
        bam="bam/input/{sample}.bam"
    params:
        cores=config.resources.threads,
        ref=config.core.ref,
    run:
        def _run(input, output, wildcards, params):
            shell(f"bwa mem -t {params.cores} {params.ref} {input.read1} {input.read2} > {output.bam}")
        _run(input, output, wildcards, params)

rule bam_sort:
    input:
        bam="bam/input/{sample}.bam",
    output:
        sortbam="bam/process/{sample}_sorted.bam",
    run:
        def _run(input, output, wildcards, params):
            shell(f"samtools sort -n --output-fmt BAM {input.bam} -T ./tmp > {output.sortbam}")
        _run(input, output, wildcards, params)

rule bam_rmdups:
    input:
        sortbam="bam/process/{sample}_sorted.bam",
    output:
        rmdupbam="bam/process/{sample}_sorted.rmdup.bam",
    run:
        def _run(input, output, wildcards, params):
            # shell(f"samtools rmdup --output-fmt BAM {input.sortbam} {output.rmdupbam}")
            fixm_tmp = f"bam/process/{wildcards.sample}_sorted.fixm.bam"
            shell(f"samtools fixmate -m --output-fmt BAM {input.sortbam} {fixm_tmp}")
            shell(f"samtools sort  --output-fmt BAM {fixm_tmp} > {input.sortbam}")
            shell(f"samtools markdup -r --output-fmt BAM {input.sortbam} {output.rmdupbam}")
            shell(f"rm {fixm_tmp}")
        _run(input, output, wildcards, params)

rule bam_addrg:
    input:
        rmdupbam="bam/process/{sample}_sorted.rmdup.bam",
    output:
        rgbam="bam/process/{sample}_sorted.rmdup.rg.bam",
    run:
        def _run(input, output, wildcards, params):
            shell(f"samtools addreplacerg --output-fmt BAM -r ID:S1 -r LB:L1 -r SM:{wildcards.sample} -o {output.rgbam} {input.rmdupbam}")
        _run(input, output, wildcards, params)

rule bam_index:
    input:
        rgbam="bam/process/{sample}_sorted.rmdup.rg.bam",
    output:
        final_bam="bam/final/{sample}_processed.bam",
        bai="bam/final/{sample}_processed.bam.bai",
    run:
        def _run(input, output, wildcards, params):
            shell(f"cp {input.rgbam} {output.final_bam}")
            shell(f"samtools index {output.final_bam}")
        _run(input, output, wildcards, params)

rule samtools_depth:
    input:
        final_bam="bam/final/{sample}_processed.bam"
    output:
        depth="depth/{sample}_depth.txt.gz"
    run:
        def _run(input, output, wildcards, params):
            shell(f"samtools depth -a {input.final_bam} | gzip > {output.depth}")
        _run(input, output, wildcards, params)

# sg.add_command(f"gatk --java-options \"-Xmx24g\" HaplotypeCaller -R {asm_path} -I {bam} -O {bam.replace('.dedupped.sorted.addrg.bam', '.g.vcf')} -ERC GVCF -A DepthPerAlleleBySample -A MappingQuality -A LikelihoodRankSumTest")
rule haplotype_caller:
    input:
        final_bam="bam/final/{sample}_processed.bam",
    output:
        gvcf="gvcf/{sample}.g.vcf",
    params:
        memory=config.resources.memory,
        ref=config.core.ref,
    run:
        def _run(input, output, wildcards, params):
            shell(f"gatk --java-options \"-Xmx{params.memory}g\" HaplotypeCaller -R {params.ref} -I {input.final_bam} -O {output.gvcf} -ERC GVCF -A DepthPerAlleleBySample -A MappingQuality -A LikelihoodRankSumTest")
        _run(input, output, wildcards, params)

rule hardfilter_gvcfs:
    input:
        gvcf="gvcf/{sample}.g.vcf",
    output:
        gvcf_filterFlagged="gvcf/{sample}_filterFlagged.g.vcf",
        gvcf_filterRemoved="gvcf/{sample}_filterRemoved.g.vcf",
    params:
        memory=config.resources.memory,
        ref=config.core.ref,
        mqrs_min=config.vcf_filter.mqrs_min,
        dp_min=config.vcf_filter.mqrs_min,
    run:
        def _run(input, output, wildcards, params):
            shell(f"gatk VariantFiltration -R {params.ref} -V {input.gvcf} -O {output.gvcf_filterFlagged} --filter-name 'mqrs_filt' --filter-expression 'MQRankSum < {params.mqrs_min}' --filter-name 'dp_filt' --filter-expression 'DP < {params.dp_min}'")
            shell(f"vcftools --vcf {output.gvcf_filterFlagged} --stdout --remove-filtered-all --recode-INFO-all --recode > {output.gvcf_filterRemoved}")
        _run(input, output, wildcards, params)

rule depth_plot:
    input:
        depth="depth/{sample}_depth.txt.gz"
    output:
        plt="fig/{sample}_depth_plot.png"
    run:
        coords = pd.read_csv(input.depth, sep="\t", compression= "gzip", header=None)
        coords.columns = ["chrom", "pos", "depth"]
        chroms = coords.chrom.unique()
        panel = Paneler(len(chroms), 1, (10, 10))
        for chrom in chroms:
            these_coords = coords[coords.chrom==chrom]
            x = these_coords.pos.to_numpy()
            y = these_coords.depth.to_numpy()
            y = np.log10(y+1)
            panel.next_ax().plot(x,y, linewidth = 0.2)
            panel.current_ax.set_title(chrom)

        panel.fig.savefig(output.plt)

rule combine_gvcfs:
    input:
        gvcfs_filtered=[f"gvcf/{sample}_filterRemoved.g.vcf" for sample in SAMPLES]
    output:
        combined_gvcf="combined/combined.g.vcf"
    params:
        memory=config.resources.memory,
        ref=config.core.ref,
    run:
        def _run(input, output, wildcards, params):
            variants = " ".join([f"--variant {gvcf}" for gvcf in input.gvcfs_filtered])
            shell(f"gatk --java-options \"-Xmx{params.memory}g\" CombineGVCFs -R {params.ref} {variants} -O {output.combined_gvcf}")
        _run(input, output, wildcards, params)

# rule genotype_gvcf:
#     input:
#         combined_gvcf="combined/combined.g.vcf"
#     output:
#         combined_vcf="combined/{combined}.vcf",
#     params:
#         memory=config.resources.memory,
#         ref=config.core.ref,
#     run:
#         def _run(input, output, wildcards, params):
#             shell(f"gatk --java-options \"-Xmx{params.memory}g\" GenotypeGVCFs -R {params.ref} -O {output.combined_vcf} -V {input.combined_gvcf}")
#         _run(input, output, wildcards, params)
#
        
